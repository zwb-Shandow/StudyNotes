# 基于CUDA实现异构并行计算

 - 了解异构计算体系结构
 - 认识并行编程的范式转变
 - 掌握GPU编程的基本要素
 - 了解CPU和GPU编程之间的差异

随着新技术和新流程的普及，高性能计算（HPC）的前景总是在变化，HPC的定义也随之变化。一般来说，它指的是使用多个处理器或计算机以高吞吐量和高效率同时完成复杂任务。通常认为HPC不仅是一种计算体系结构，而且是一组元素，包括硬件系统、软件工具、编程平台和并行编程范式。
在过去的十年里，高性能计算有了显著的发展，特别是因为GPU-CPU异构架构的出现，这导致了并行编程中有趣的重大范式转变。本章开始您对异构并行编程的理解。

## 并行计算
在过去的几十年里，人们对并行计算越来越感兴趣。并行计算的主要目标是提高计算速度。
从纯计算的角度来看，并行计算可以定义为一种计算形式，其中许多计算同时进行，其原理是大问题通常可以分为小问题，然后同时求解。
从程序员的角度来看，一个自然的问题是如何将并行计算映射到计算机上。假设您有多个计算资源。并行计算可以定义为同时使用多个计算资源（核心或计算机）来执行并行计算。一个大问题被分解成更小的问题，然后在不同的计算资源上同时解决每个更小的问题。并行计算的软件和硬件方面紧密地交织在一起。事实上，并行计算通常涉及计算技术的两个不同领域：

- 计算机架构（硬件方面）
- 并行编程（软件方面）

计算机体系结构侧重于在体系结构级别支持并行性，而并行编程侧重于通过充分利用计算机体系结构的计算能力来同时解决问题。为了在软件中实现并行执行，硬件必须提供一个支持多个进程或多个线程并行执行的平台。

大多数现代处理器实现哈佛体系结构，如图1-1所示，该体系结构由三个主要组件组成：

- 内存（指令内存和数据内存）
- CPU
- I/O 接口

高性能计算的关键部件是中央处理器（CPU），通常被称为核心。在计算机的早期，芯片上只有一个核心。该架构被称为单处理器。如今，芯片设计的趋势是将多个核集成到一个处理器上，通常称为多核，以支持体系结构级别的并行性。因此，编程可以被视为将问题的计算映射到可用核的过程，从而获得并行执行。

在实现顺序算法时，您可能不需要了解计算机体系结构的细节就可以编写正确的程序。然而，在为多核机器实现算法时，程序员更重要的是要意识到底层计算机体系结构的特征。编写正确有效的并行程序需要具备多核体系结构的基本知识。

以下部分介绍了并行计算的一些基本概念，以及这些概念与CUDA编程的关系。

## 顺序和并行编程

当用计算机程序解决问题时，很自然地会将问题划分为一系列离散的计算；每个计算都执行一个指定的任务，如图1-2所示。这样的程序称为顺序程序。

有两种方法可以对两个计算之间的关系进行分类：有些计算通过优先约束相关联，因此必须按顺序计算；其他的则没有这样的限制，并且可以同时计算。任何包含同时执行的任务的程序都是并行程序。如图1-3所示，一个并行程序可能，而且很可能会有一些顺序部分。

从程序员的角度来看，程序由两个基本组成部分组成：指令和数据。当一个计算问题被分解成许多小的计算部分时，每一部分都被称为一个任务。在任务中，各个指令消耗输入、应用函数并产生输出。当一条指令消耗前一条指令产生的数据时，就会发生数据依赖关系。因此，您可以将任意两个任务之间的关系分类为依赖任务（如果其中一个任务消耗另一个任务的输出）或独立任务。

分析数据依赖性是实现并行算法的一项基本技能，因为依赖性是并行性的主要抑制因素之一，并且理解它们对于在现代编程世界中获得应用程序加速是必要的。 在大多数情况下，多个独立的依赖任务链提供了并行化的最佳机会。

### 并行

如今，并行性变得无处不在，并行编程正在成为编程世界的主流。多层次的并行性是建筑设计的驱动力。

应用程序中并行性有两种基本类型：

- 任务并行
- 数据并行

当有许多任务或函数可以独立和在很大程度上并行操作时，就会出现任务并行性。任务并行性着重于在多个内核上分发功能。

当有许多数据项可以同时操作时，就会出现数据并行性。数据并行性着重于在多个核心上分配数据。

CUDA编程特别适合于解决可以表示为数据并行计算的问题。本书的主要焦点是如何用CUDA编程来解决数据并行问题。许多处理大型数据集的应用程序可以使用数据并行模型来加快计算速度。数据并行处理将数据元素映射到并行线程。

设计数据并行程序的第一步是跨线程划分数据，每个线程处理一部分数据。通常，有两种方法可以对数据进行分区：块分区和循环分区。在块分区中，许多连续的数据元素被分块在一起。每个区块按任何顺序分配给单个线程，线程通常一次只处理一个区块。在循环分区中，将更少的数据元素连接在一起。相邻线程接收相邻的块，并且每个线程可以处理多个块。为线程选择一个新的块来处理意味着要向前跳尽可能多的块。

## 计算机体系架构

## 异构计算

在最早的日子里，计算机只包含中央处理器（CPU），用于运行通用编程任务。自过去十年以来，高性能计算社区的主流计算机一直在转向包括其他处理元件。最流行的是GPU，最初设计用于并行执行专门的图形计算。随着时间的推移，GPU变得更加强大和通用，使其能够以优异的性能和高功率效率应用于通用并行计算任务。

通常，CPU和GPU是通过PCI Express总线连接在单个计算节点内的离散处理组件。在这种类型的体系结构中，GPU被称为分立设备。

从同构系统到异构系统的转变是高性能计算史上的一个里程碑。同质计算使用一个或多个相同体系结构的处理器来执行应用程序。相反，异构计算使用一套处理器体系结构来执行应用程序，将任务应用于它们非常适合的体系结构，从而提高性能。

尽管与传统的高性能计算系统相比，异构系统提供了显著的优势，但目前应用程序设计复杂性的增加限制了此类系统的有效使用。虽然并行编程最近受到了广泛的关注，但包含异构资源增加了复杂性。

如果您是并行编程的新手，那么您可以从异构体系结构上提供的性能改进和高级软件工具中获益。如果你已经是一个优秀的并行程序员，那么适应异构体系结构上的并行编程是很简单的

### 异构体系结构

如今，一个典型的异构计算节点由两个多核CPU和两个或多个多核GPU组成。GPU目前不是一个独立的平台，而是CPU的协同处理器。因此，GPU必须通过PCI Express总线与基于CPU的主机协同工作，如图1-9所示。这就是为什么在GPU计算术语中，CPU被称为主机，GPU被称为设备。

异构应用程序由两部分组成：

- 主机代码
- 设备代码

主机代码在CPU上运行，设备代码在GPU上运行。在异构平台上执行的应用程序通常由CPU初始化。CPU代码负责在设备上加载计算密集型任务之前管理设备的环境、代码和数据。

对于计算密集型应用程序，程序部分通常表现出大量的数据并行性。 GPU用于加速这部分数据并行的执行。 当物理上与 CPU 分离的硬件组件用于加速应用程序的计算密集型部分时，它被称为硬件加速器。 GPU 可以说是硬件加速器最常见的例子。

NVIDIA的GPU计算平台已在以下产品系列中启用：

- Tegra
- GeForce
- Quadro
- Tesla

有两个描述GPU功能的重要特征：

- CUDA核心数量
- 内存大小

因此，有两个不同的指标来描述GPU性能：

- 峰值计算性能
- 内存带宽

峰值计算性能是计算能力的衡量标准，通常定义为每秒可以处理多少单精度或双精度浮点计算。峰值性能通常用gflops（每秒十亿次浮点运算）或tflops（每秒钟万亿次浮点计算）表示。内存带宽是衡量数据从内存读取或存储到内存的比率的指标。内存带宽通常以每秒千兆字节（GB/s）表示。表1-1简要总结了费米和开普勒的结构和性能特点。

### 异构计算范式

GPU计算并不意味着要取代CPU计算。对于某些类型的程序，每种方法都有优势。CPU计算有利于控制密集型任务，GPU计算有利于数据并行计算密集型任务。当CPU由GPU补充时，它将成为一个强大的组合。CPU针对以短序列计算操作和不可预测的控制流为标志的动态工作负载进行了优化；GPU瞄准了光谱的另一端：由具有简单控制流的计算任务主导的工作负载。如图1-10所示，有两个维度区分CPU和GPU的应用范围：

- 并行度
- 数据大小

如果问题具有较小的数据大小、复杂的控制逻辑和/或低级别并行性，则CPU是一个不错的选择，因为它能够处理复杂的逻辑和指令级并行性。如果手头的问题处理大量数据并表现出巨大的数据并行性，那么GPU是正确的选择，因为它有大量的可编程内核，可以支持多线程，并且与CPU相比具有更大的峰值带宽。

CPU+GPU异构并行计算架构的发展是因为CPU和GPU具有互补的属性，使应用程序能够使用这两种类型的处理器来实现最佳性能。因此，为了获得最佳性能，您可能需要同时使用CPU和GPU进行应用，在CPU上执行顺序部分或任务并行部分，在GPU上执行密集数据并行部分，如图1-11所示。

以这种方式编写代码可以确保GPU和CPU的特性互补，从而充分利用CPU+GPU组合系统的计算能力。为了支持应用程序的CPU+GPU联合执行，NVIDIA设计了一个名为CUDA的编程模型。这个新的编程模型是本书其余部分的重点。

> CPU线程与GPU线程
>
> CPU上的线程通常是重量级实体。操作系统必须在CPU执行通道上和CPU执行通道下交换线程，以提供多线程功能。上下文切换既缓慢又昂贵。
>
> GPU上的线程非常轻。在一个典型的系统中，成千上万的线程排队等待工作。如果GPU必须等待一组线程，它只需开始在另一组线程上执行工作。
>
> CPU核心设计用于最大程度地减少一个或两个线程的延迟，而GPU核心旨在处理大量并发，轻量重量的线程，以最大程度地提高吞吐量。
>
> 如今，一个拥有四个四核处理器的CPU只能同时运行16个线程，如果CPU支持超线程，则只能运行32个线程。
>
> 现代NVIDIA GPU每个多处理器最多可同时支持1536个活动线程。在具有16个多处理器的GPU上，这将导致24000多个并发活动线程。





GPU编程模型：

- 核函数
- 内存管理
- 线程管理
- 流

CUDA应用开发宏观层次：

- 领域层
- 逻辑层
- 硬件层

### 线程管理

一个核函数只能有一个grid，一个grid可以有很多个block，每个block可以有很多的thread。

层次关系：

- grid
- block：同一block中的线程可以实现同步及共享内存；不同block中的线程是**物理隔离**的
- thread：依靠 blockIdx(block在grid中的位置索引)及 threadIdx(thread在block中的位置索引)确定线程标号

线程层次关系：

- 当内核启动时，CUDA 会生成一个以三维层次结构组织的线程网格
  - 每个网格被组织成线程块或块的数组
  - 每个块最多可以包含 1024 个线程
  - 块中的线程数在 `blockDim` 变量中给出
  - 线程块的尺寸应该是32的倍数
- 块中的每个线程都有唯一的 `threadIdx` 值
  - 组合 threadIdx 和 blockIdx 值以创建唯一的全局索引

threadIdx 随着 Grid 和 Block 的划分方式的不同而变化。

### 核函数

语法：

```C
__global__ void kernel_name(argument list);
```

#### 核函数性能测试

- CPU计时器
- GPU计时器
- NVIDIA profiler(命令行工具)

#### profiler 使用

```bash
nvprof [nvprof_args] <application> [application_args]
```

> 比较应用程序性能以最大限度地提高理论极限
>
> 在执行应用程序优化时，必须确定应用程序与理论限制的比较非常重要。从 nvprof 收集的计数器可以帮助您为应用程序得出指令和内存吞吐量。如果将应用程序的值与理论峰值进行比较，则可以确定应用程序是否受算术或内存带宽的限制。

### Q&A

1. 核函数的 grid 与 block 如何确定已保证最佳性能
